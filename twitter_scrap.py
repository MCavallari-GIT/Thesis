# -*- coding: utf-8 -*-
"""Twitter_Scrap.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XVAfFC40Cij-hHKuDMSSJQrXI5rrv-hE
"""

from datetime import datetime,timedelta
import os
import requests
import json
import pandas as pd
from tqdm.notebook import tqdm
import time
import csv
import dateutil.parser


from sentence_transformers import SentenceTransformer
from numpy import dot
from numpy.linalg import norm

def create_headers(bearer_token):
    headers = {"Authorization": "Bearer {}".format(bearer_token)}
    return headers

def create_url(timeline, start_date, hour_delta, max_results = 10):
    
    search_url = "https://api.twitter.com/2/tweets/search/all" #Change to the endpoint you want to collect data from

    time=dt = datetime.strptime(start_date, '%Y-%m-%dT%H:%M:%S.%fZ')
    end_date=dt + timedelta(hours=hour_delta)

    #change params based on the endpoint you are using
    query_params = {'query': 'from:{}'.format(timeline),
                    'start_time':start_date,
                    'end_time': end_date.isoformat() + "Z",
                    'max_results': max_results,
                    'tweet.fields': 'id,text,author_id,created_at,source',
                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',
                    'next_token': {}}
    return (search_url, query_params)

def append_to_csv(json_response, fileName):

    #A counter variable
    counter = 0

    #Open OR create the target CSV file
    csvFile = open(fileName, "a", newline="", encoding='utf-8')
    csvWriter = csv.writer(csvFile)
    #csvWriter.writerow(["authorID","created_at","TweetID","Text"])
    #Loop through each tweet
    for tweet in json_response['data']:
        
        # We will create a variable for each since some of the keys might not exist for some tweets
        # So we will account for that

        # 1. Author ID
        author_id = tweet['author_id']

        # 2. Time created
        created_at = dateutil.parser.parse(tweet['created_at'])

        # 3. Tweet ID
        tweet_id = tweet['id']

        # 5. Tweet text
        text = tweet['text']
        
        # Assemble all data in a list
        res = [author_id, created_at, tweet_id, text]
        
        # Append the result to the CSV file
        csvWriter.writerow(res)
        counter += 1

    # When done, close the CSV file
    csvFile.close()
    # Print the number of tweets for this iteration
    print("# of Tweets added from this response: ", counter)

def connect_to_endpoint(url, headers, params, next_token = None):
    params['next_token'] = next_token   #params object received from create_url function
    response = requests.request("GET", url, headers = headers, params = params)
    print("Endpoint Response Code: " + str(response.status_code))
    if response.status_code != 200:
        raise Exception(response.status_code, response.text)
    return response.json()

def read_csv(filename):
  data = pd.read_csv(filename,header=0)
  return data

def retrive(bearer_token,timelines,filename="data.csv",mr=10):
  # Create file
  csvFile = open(filename, "a", newline="", encoding='utf-8')
  csvWriter = csv.writer(csvFile)
  
  #Create headers for the data you want to save, in this example, we only want save these columns in our dataset
  csvWriter.writerow(["authorID","created_at","TweetID","Text"])
  csvFile.close()

  for timeline in timelines:
    print(timeline)
    headers=create_headers(bearer_token)
    url=create_url(timeline, start_time, hour_delta=delta, max_results = mr)
    json_response = connect_to_endpoint(url[0], headers, url[1])
    append_to_csv(json_response, filename)
    time.sleep(2)
  
  #df = pd.read_csv(filename,header=None)
  #df.rename(columns={'0': 'Custom field (Verified Date)'}, inplace=True)

def retrive_many(bearer_token,timelines,tweets_per_timeline,mr,start_time,delta,filename="data.csv",limit_fixed=False):#max result per timeline
  total_tweets = 0
  headers=create_headers(bearer_token)
  # Create file
  csvFile = open(filename, "a", newline="", encoding='utf-8')
  csvWriter = csv.writer(csvFile)
  #Create headers for the data you want to save, in this example, we only want save these columns in our dataset
  csvWriter.writerow(["authorID","created_at","TweetID","Text"])
  csvFile.close()

  for timeline in timelines:
    print("*****************************",timeline,"**************************")
    count=0
    next_token = None
    flag=True
    while flag:
        # Check if max_count reached
        if limit_fixed==True:
          if count >= tweets_per_timeline:
              break
        print("-------------------")
        print("Token: ", next_token)
        url=create_url(timeline, start_time, hour_delta=delta, max_results = mr)
        json_response = connect_to_endpoint(url[0], headers, url[1],next_token)
        result_count = json_response['meta']['result_count']

        if 'next_token' in json_response['meta']:
            # Save the token to use for next call
            next_token = json_response['meta']['next_token']
            print("Next Token: ", next_token)       
        # If no next token exists
        else:
          #Since this is the final request, turn flag to false to move to the next time period.
          flag = False
          next_token = None
        if result_count is not None and result_count > 0:
          print("-------------------")
          append_to_csv(json_response, filename)
          count += result_count
          total_tweets += result_count
          print("Total # of Tweets added: ", total_tweets)
          print("-------------------")
          time.sleep(5)
        time.sleep(5)

"""**TEXT SIMILARITY**"""

def sort_tuples(tuple_list):
  sorted_result=sorted(tuple_list, key=lambda x: x[1],reverse=True)
  return sorted_result

def similarity(query,corpus):
  model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')
  results=[]
  for tweet in tqdm(corpus):
    couple=(query,tweet)
    embeddings = model.encode(couple)
    cos_sim = dot(embeddings[0], embeddings[1])/(norm(embeddings[0])*norm(embeddings[1]))
    results.append((tweet,cos_sim))
  return sort_tuples(results)

#query="Defending world champions end Moroccoâ€™s World Cup dreams and book their place in the final. France have beaten Morocco 2-0 at Al Bayt Stadium to claim a place in Sundayâ€™s World Cup final."
#results=similarity(query,df['Text'].tolist())

#x=list(filter(lambda x: x[1] >= 0.5, results))

