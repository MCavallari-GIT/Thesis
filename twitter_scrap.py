# -*- coding: utf-8 -*-
"""Twitter_Scrap.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XVAfFC40Cij-hHKuDMSSJQrXI5rrv-hE
"""

from datetime import datetime,timedelta
import os
import requests
import json
import pandas as pd
from tqdm.notebook import tqdm
import time
import csv
import dateutil.parser

def create_headers(bearer_token):
    headers = {"Authorization": "Bearer {}".format(bearer_token)}
    return headers

def create_url(timeline, start_date, hour_delta, max_results = 10):
    
    search_url = "https://api.twitter.com/2/tweets/search/all" #Change to the endpoint you want to collect data from

    time=dt = datetime.strptime(start_date, '%Y-%m-%dT%H:%M:%S.%fZ')
    end_date=dt + timedelta(hours=hour_delta)

    #change params based on the endpoint you are using
    query_params = {'query': 'from:{}'.format(timeline),
                    'start_time':start_date,
                    'end_time': end_date.isoformat() + "Z",
                    'max_results': max_results,
                    'tweet.fields': 'id,text,author_id,created_at,source',
                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',
                    'next_token': {}}
    return (search_url, query_params)

def append_to_csv(json_response, fileName):

    #A counter variable
    counter = 0

    #Open OR create the target CSV file
    csvFile = open(fileName, "a", newline="", encoding='utf-8')
    csvWriter = csv.writer(csvFile)
    #csvWriter.writerow(["authorID","created_at","TweetID","Text"])
    #Loop through each tweet
    for tweet in json_response['data']:
        
        # We will create a variable for each since some of the keys might not exist for some tweets
        # So we will account for that

        # 1. Author ID
        author_id = tweet['author_id']

        # 2. Time created
        created_at = dateutil.parser.parse(tweet['created_at'])

        # 3. Tweet ID
        tweet_id = tweet['id']

        # 5. Tweet text
        text = tweet['text']
        
        # Assemble all data in a list
        res = [author_id, created_at, tweet_id, text]
        
        # Append the result to the CSV file
        csvWriter.writerow(res)
        counter += 1

    # When done, close the CSV file
    csvFile.close()
    # Print the number of tweets for this iteration
    print("# of Tweets added from this response: ", counter)

def connect_to_endpoint(url, headers, params, next_token = None):
    params['next_token'] = next_token   #params object received from create_url function
    response = requests.request("GET", url, headers = headers, params = params)
    print("Endpoint Response Code: " + str(response.status_code))
    if response.status_code != 200:
        raise Exception(response.status_code, response.text)
    return response.json()

def read_csv(filename):
  data = pd.read_csv(filename,header=0)
  return data

def retrive(bearer_token,timelines,filename="data.csv",mr=10):
  # Create file
  csvFile = open(filename, "a", newline="", encoding='utf-8')
  csvWriter = csv.writer(csvFile)
  
  #Create headers for the data you want to save, in this example, we only want save these columns in our dataset
  csvWriter.writerow(["authorID","created_at","TweetID","Text"])
  csvFile.close()

  for timeline in timelines:
    print(timeline)
    headers=create_headers(bearer_token)
    url=create_url(timeline, start_time, hour_delta=delta, max_results = mr)
    json_response = connect_to_endpoint(url[0], headers, url[1])
    append_to_csv(json_response, filename)
    time.sleep(2)
  
  #df = pd.read_csv(filename,header=None)
  #df.rename(columns={'0': 'Custom field (Verified Date)'}, inplace=True)

def retrive_many(bearer_token,timelines,tweets_per_timeline,mr,start_time,delta,filename="data.csv",limit_fixed=False):#max result per timeline
  total_tweets = 0
  headers=create_headers(bearer_token)
  # Create file
  csvFile = open(filename, "a", newline="", encoding='utf-8')
  csvWriter = csv.writer(csvFile)
  #Create headers for the data you want to save, in this example, we only want save these columns in our dataset
  csvWriter.writerow(["authorID","created_at","TweetID","Text"])
  csvFile.close()

  for timeline in timelines:
    print("*****************************",timeline,"**************************")
    count=0
    next_token = None
    flag=True
    while flag:
        # Check if max_count reached
        if limit_fixed==True:
          if count >= tweets_per_timeline:
              break
        print("-------------------")
        print("Token: ", next_token)
        url=create_url(timeline, start_time, hour_delta=delta, max_results = mr)
        json_response = connect_to_endpoint(url[0], headers, url[1],next_token)
        result_count = json_response['meta']['result_count']

        if 'next_token' in json_response['meta']:
            # Save the token to use for next call
            next_token = json_response['meta']['next_token']
            print("Next Token: ", next_token)
            if result_count is not None and result_count > 0 and next_token is not None:
                print("Start Date: ", start_time)
                append_to_csv(json_response, filename)
                count += result_count
                total_tweets += result_count
                print("Total # of Tweets added: ", total_tweets)
                print("-------------------")
                time.sleep(5)                
        # If no next token exists
        else:
            if result_count is not None and result_count > 0:
                print("-------------------")
                print("Start Date: ", start_time)
                append_to_csv(json_response, filename)
                count += result_count
                total_tweets += result_count
                print("Total # of Tweets added: ", total_tweets)
                print("-------------------")
                time.sleep(5)
            
            #Since this is the final request, turn flag to false to move to the next time period.
            flag = False
            next_token = None
        time.sleep(5)

